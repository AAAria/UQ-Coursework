{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  holiday    temp  rain_1h  snow_1h  clouds_all weather_main  \\\n",
      "0    None  288.28      0.0      0.0          40       Clouds   \n",
      "1    None  289.36      0.0      0.0          75       Clouds   \n",
      "2    None  289.58      0.0      0.0          90       Clouds   \n",
      "3    None  290.13      0.0      0.0          90       Clouds   \n",
      "4    None  291.14      0.0      0.0          75       Clouds   \n",
      "\n",
      "  weather_description            date_time  traffic_volume  year  ...  \\\n",
      "0    scattered clouds  2012-10-02 09:00:00            5545  2012  ...   \n",
      "1       broken clouds  2012-10-02 10:00:00            4516  2012  ...   \n",
      "2     overcast clouds  2012-10-02 11:00:00            4767  2012  ...   \n",
      "3     overcast clouds  2012-10-02 12:00:00            5026  2012  ...   \n",
      "4       broken clouds  2012-10-02 13:00:00            4918  2012  ...   \n",
      "\n",
      "   cloud_grade  fes-mean  history_mean  history_mean-w  history_max  \\\n",
      "0            4       0.0   5314.000000     5137.368421         5877   \n",
      "1            7       0.0   4455.545455     4480.663462         4909   \n",
      "2            9       0.0   4639.526316     4705.712871         4947   \n",
      "3            9       0.0   4865.227273     4937.694444         5178   \n",
      "4            7       0.0   4822.500000     4900.687500         5130   \n",
      "\n",
      "   history_min  history_med  weather_mean  history_mean_h  history_mean_h1  \n",
      "0         4684       5391.0   5018.146199     5027.848485      5007.414354  \n",
      "1         4033       4480.0   4348.759358     4803.023923      4745.968900  \n",
      "2         4151       4739.0   4520.253012     4653.433014      4819.359809  \n",
      "3         4370       4985.0   4748.597484     4775.751196      4785.336279  \n",
      "4         4380       4895.0   4780.480519     4943.869875      5031.909006  \n",
      "\n",
      "[5 rows x 56 columns]\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor as SVR\n",
    "from sklearn.preprocessing import StandardScaler as SSC\n",
    "from sklearn.preprocessing import MinMaxScaler as MMS\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb \n",
    "\n",
    "# 读入数据，此处为要读入文件的路径 \n",
    "filename = 'Metro_Interstate_Traffic_Volume.csv'\n",
    "#通过csv模块中的read_csv直接读取文件\n",
    "data = read_csv (filename)\n",
    "\n",
    "# 即将创建一个节假日的列表，通过这样的一个列表我们可以将数据中的节假日和普通时间区分开来\n",
    "holiday_dict = [];\n",
    "\n",
    "#数据中出现的2018年的节假日，将判定为节假日的时间录入列表\n",
    "'''\n",
    "在这个列表中，我们要将假期和他的持续时间都记录下来。例1.1号为元旦，元旦假期一共有三天，为1.1，1.2，1.3 。虽然在所给的数据中，\n",
    "仅将1.1算为了节日，但我们仍将1.2和1.3日记录为节日。这样的做法的意义是节日的流量和平时是不一样的，假期的流量可以理解为异常值，\n",
    "只有将假期和平时彻底分开，才能分别预测假期和平时，同时也减少异常值对模型的影响。\n",
    "\n",
    "'''\n",
    "holiday_dict.append([2018,1,1])\n",
    "holiday_dict.append([2018,1,2])\n",
    "holiday_dict.append([2018,1,3])\n",
    "holiday_dict.append([2018,1,16])\n",
    "holiday_dict.append([2018,1,17])\n",
    "holiday_dict.append([2018,1,18])\n",
    "holiday_dict.append([2018,2,13])\n",
    "holiday_dict.append([2018,2,14])\n",
    "holiday_dict.append([2018,2,15])\n",
    "holiday_dict.append([2018,3,14])\n",
    "holiday_dict.append([2018,3,18])\n",
    "holiday_dict.append([2018,5,28])\n",
    "holiday_dict.append([2018,5,29])\n",
    "holiday_dict.append([2018,5,30])\n",
    "holiday_dict.append([2018,6,11])\n",
    "holiday_dict.append([2018,7,2])\n",
    "holiday_dict.append([2018,7,3])\n",
    "holiday_dict.append([2018,7,4])\n",
    "holiday_dict.append([2018,9,2])\n",
    "holiday_dict.append([2018,9,3])\n",
    "holiday_dict.append([2018,9,4])\n",
    "holiday_dict.append([2018,9,5])\n",
    "holiday_dict.append([2018,10,8])\n",
    "holiday_dict.append([2018,10,9])\n",
    "holiday_dict.append([2018,10,10])\n",
    "holiday_dict.append([2018,11,11])\n",
    "holiday_dict.append([2018,11,12])\n",
    "holiday_dict.append([2018,11,13])\n",
    "holiday_dict.append([2018,11,24])\n",
    "holiday_dict.append([2018,11,26])\n",
    "holiday_dict.append([2018,11,27])\n",
    "holiday_dict.append([2018,12,24])\n",
    "holiday_dict.append([2018,12,25])\n",
    "holiday_dict.append([2018,12,26])\n",
    "\n",
    "#此处为2017年的假期时间，在这里我们记录了2017年的假期\n",
    "holiday_dict.append([2017,1,1])\n",
    "holiday_dict.append([2017,1,2])\n",
    "holiday_dict.append([2017,1,3])\n",
    "holiday_dict.append([2017,1,16])\n",
    "holiday_dict.append([2017,1,17])\n",
    "holiday_dict.append([2017,1,18])\n",
    "holiday_dict.append([2017,2,13])\n",
    "holiday_dict.append([2017,2,14])\n",
    "holiday_dict.append([2017,2,15])\n",
    "holiday_dict.append([2017,3,14])\n",
    "holiday_dict.append([2017,3,18])\n",
    "holiday_dict.append([2017,5,28])\n",
    "holiday_dict.append([2017,5,29])\n",
    "holiday_dict.append([2017,5,30])\n",
    "holiday_dict.append([2017,6,11])\n",
    "holiday_dict.append([2017,7,2])\n",
    "holiday_dict.append([2017,7,3])\n",
    "holiday_dict.append([2017,7,4])\n",
    "holiday_dict.append([2017,9,2])\n",
    "holiday_dict.append([2017,9,3])\n",
    "holiday_dict.append([2017,9,4])\n",
    "holiday_dict.append([2017,9,5])\n",
    "holiday_dict.append([2017,10,8])\n",
    "holiday_dict.append([2017,10,9])\n",
    "holiday_dict.append([2017,10,10])\n",
    "holiday_dict.append([2017,11,11])\n",
    "holiday_dict.append([2017,11,12])\n",
    "holiday_dict.append([2017,11,13])\n",
    "holiday_dict.append([2017,11,24])\n",
    "holiday_dict.append([2017,11,26])\n",
    "holiday_dict.append([2017,11,27])\n",
    "holiday_dict.append([2017,12,24])\n",
    "holiday_dict.append([2017,12,25])\n",
    "holiday_dict.append([2017,12,26])\n",
    "\n",
    "# 此处为2016年的假期时间，在这里我们记录了2016年的假期\n",
    "holiday_dict.append([2016,1,1])\n",
    "holiday_dict.append([2016,1,2])\n",
    "holiday_dict.append([2016,1,3])\n",
    "holiday_dict.append([2016,1,16])\n",
    "holiday_dict.append([2016,1,17])\n",
    "holiday_dict.append([2016,1,18])\n",
    "holiday_dict.append([2016,2,13])\n",
    "holiday_dict.append([2016,2,14])\n",
    "holiday_dict.append([2016,2,15])\n",
    "holiday_dict.append([2016,3,14])\n",
    "holiday_dict.append([2016,3,18])\n",
    "holiday_dict.append([2016,5,28])\n",
    "holiday_dict.append([2016,5,29])\n",
    "holiday_dict.append([2016,5,30])\n",
    "holiday_dict.append([2016,6,11])\n",
    "holiday_dict.append([2016,7,2])\n",
    "holiday_dict.append([2016,7,3])\n",
    "holiday_dict.append([2016,7,4])\n",
    "holiday_dict.append([2016,9,2])\n",
    "holiday_dict.append([2016,9,3])\n",
    "holiday_dict.append([2016,9,4])\n",
    "holiday_dict.append([2016,9,5])\n",
    "holiday_dict.append([2016,10,8])\n",
    "holiday_dict.append([2016,10,9])\n",
    "holiday_dict.append([2016,10,10])\n",
    "holiday_dict.append([2016,11,11])\n",
    "holiday_dict.append([2016,11,12])\n",
    "holiday_dict.append([2016,11,13])\n",
    "holiday_dict.append([2016,11,24])\n",
    "holiday_dict.append([2016,11,26])\n",
    "holiday_dict.append([2016,11,27])\n",
    "holiday_dict.append([2016,12,24])\n",
    "holiday_dict.append([2016,12,25])\n",
    "holiday_dict.append([2016,12,26])\n",
    "\n",
    "# 此处为2013年的假期时间，在这里我们记录了2013年的假期\n",
    "holiday_dict.append([2013,1,1])\n",
    "holiday_dict.append([2013,1,2])\n",
    "holiday_dict.append([2013,1,3])\n",
    "holiday_dict.append([2013,2,16])\n",
    "holiday_dict.append([2013,2,17])\n",
    "holiday_dict.append([2013,2,18])\n",
    "holiday_dict.append([2013,5,25])\n",
    "holiday_dict.append([2013,5,26])\n",
    "holiday_dict.append([2013,5,27])\n",
    "holiday_dict.append([2013,7,2])\n",
    "holiday_dict.append([2013,7,3])\n",
    "holiday_dict.append([2013,7,4])\n",
    "holiday_dict.append([2013,8,22])\n",
    "holiday_dict.append([2013,8,31])\n",
    "holiday_dict.append([2013,9,1])\n",
    "holiday_dict.append([2013,9,2])\n",
    "holiday_dict.append([2013,10,12])\n",
    "holiday_dict.append([2013,10,13])\n",
    "holiday_dict.append([2013,10,14])\n",
    "holiday_dict.append([2013,11,9])\n",
    "holiday_dict.append([2013,11,10])\n",
    "holiday_dict.append([2013,11,11])\n",
    "holiday_dict.append([2013,11,28])\n",
    "holiday_dict.append([2013,11,24])\n",
    "holiday_dict.append([2013,11,25])\n",
    "\n",
    "#此处为2014年的假期时间，在这里我们记录了2014年的假期\n",
    "holiday_dict.append([2014,1,1])\n",
    "holiday_dict.append([2014,1,2])\n",
    "holiday_dict.append([2014,1,3])\n",
    "holiday_dict.append([2014,1,18])\n",
    "holiday_dict.append([2014,1,19])\n",
    "holiday_dict.append([2014,1,20])\n",
    "holiday_dict.append([2014,2,15])\n",
    "holiday_dict.append([2014,2,16])\n",
    "holiday_dict.append([2014,2,17])\n",
    "holiday_dict.append([2014,5,24])\n",
    "holiday_dict.append([2014,5,25])\n",
    "holiday_dict.append([2014,5,26])\n",
    "\n",
    "#此处为2015年的假期时间，在这里我们记录了2015年的假期\n",
    "holiday_dict.append([2015,7,3])\n",
    "holiday_dict.append([2015,7,4])\n",
    "holiday_dict.append([2015,7,5])\n",
    "holiday_dict.append([2015,8,27])\n",
    "holiday_dict.append([2015,9,5])\n",
    "holiday_dict.append([2015,9,6])\n",
    "holiday_dict.append([2015,9,7])\n",
    "holiday_dict.append([2015,10,10])\n",
    "holiday_dict.append([2015,10,11])\n",
    "holiday_dict.append([2015,10,12])\n",
    "holiday_dict.append([2015,11,11])\n",
    "holiday_dict.append([2015,11,26])\n",
    "holiday_dict.append([2015,12,24])\n",
    "holiday_dict.append([2015,12,25])\n",
    "holiday_dict.append([2015,12,26])\n",
    "holiday_dict.append([2015,12,27])\n",
    "\n",
    "'''\n",
    "在2013，2014，2015三个时间段，可以发现我们对于假期的录入偏少，这是因为在数据集中，这三年的数据是有缺失的。\n",
    "在这种情况下，我们现在只考虑已经给出来的数据，而不是去臆想数据。因此这里的数据量偏少也是合理的。\n",
    "'''\n",
    "# 创建节日列表\n",
    "'''\n",
    "在这个列表中，我们记录了每个节日开始的时间。不管是训练集还是测试集，我们都需要将节日开始的时间标记出来，\n",
    "这样的做法也和上面假期列表来配合使用，能够达到更好的预处理效果。\n",
    "'''\n",
    "festival_dict = list()\n",
    "festival_dict.append([2013,1,1 ])\n",
    "festival_dict.append([2013,2,18])\n",
    "festival_dict.append([2013,5,27])\n",
    "festival_dict.append([2013,7,4 ])\n",
    "festival_dict.append([2013,8,22])\n",
    "festival_dict.append([2013,9,2 ])\n",
    "festival_dict.append([2013,10,14])\n",
    "festival_dict.append([2013,11,11])\n",
    "festival_dict.append([2013,11,28])\n",
    "festival_dict.append([2013,12,25])\n",
    "festival_dict.append([2014,1,1])\n",
    "festival_dict.append([2014,1,20]) \n",
    "festival_dict.append([2014,2,17 ])\n",
    "festival_dict.append([2014,5,26 ])\n",
    "festival_dict.append([2015,7,3])\n",
    "festival_dict.append([2015,8,27]) \n",
    "festival_dict.append([2015,9,7])\n",
    "festival_dict.append([2015,10,12])\n",
    "festival_dict.append([2015,11,11])\n",
    "festival_dict.append([2015,11,26])\n",
    "festival_dict.append([2015,11,26])\n",
    "festival_dict.append([2015,12,25])\n",
    "festival_dict.append([2016,1,1]) \n",
    "festival_dict.append([2016,1,1 ])\n",
    "festival_dict.append([2016,2,15])\n",
    "festival_dict.append([2016,5,30])\n",
    "festival_dict.append([2016,7,4 ])\n",
    "festival_dict.append([2016,8,25])\n",
    "festival_dict.append([2016,9,5 ])\n",
    "festival_dict.append([2016,9,5 ])\n",
    "festival_dict.append([2016,10,10])\n",
    "festival_dict.append([2016,11,11])\n",
    "festival_dict.append([2016,11,24])\n",
    "festival_dict.append([2016,12,26])\n",
    "festival_dict.append([2016,12,26])\n",
    "festival_dict.append([2017,1,1])\n",
    "festival_dict.append([2017,1,2])\n",
    "festival_dict.append([2017,1,16])\n",
    "festival_dict.append([2017,5,29])\n",
    "festival_dict.append([2017,7,4])\n",
    "festival_dict.append([2017,9,4])\n",
    "festival_dict.append([2017,10,9])\n",
    "festival_dict.append([2017,12,25])\n",
    "festival_dict.append([2018,1,1])\n",
    "festival_dict.append([2018,7,4])\n",
    "festival_dict.append([2018,9,3])\n",
    "festival_dict.append([2018,1,15])\n",
    "festival_dict.append([2018,5,28])\n",
    "festival_dict.append([2018,8,23])\n",
    "festival_dict.append([2018,2,19])\n",
    "\n",
    "# 预处理部分 \n",
    "'''\n",
    "这个函数帮助我们切分数据，将原来的时间数据分离为year,month,day和hour。获得这样的时间数据可以\n",
    "帮助我们匹配对应的节假日，这样就可以区分开工作日和节假日的区别了。  我们通过数据中的符号达到分\n",
    "离的目的，其中的‘/’和‘ ’都作为了分离标志\n",
    "'''\n",
    "def get_time(data):\n",
    "    data['year']    = [int(i.split('-')[0]) for i in data['date_time'].values]\n",
    "    data['month']   = [int(i.split('-')[1]) for i in data['date_time'].values]\n",
    "    data['day']     = [int(i.split('-')[2].split(' ')[0]) for i in data['date_time'].values]\n",
    "    data['hour']    = [int(i.split(' ')[1].split(':')[0]) for i in data['date_time'].values]\n",
    "    return data\n",
    "\n",
    "data=get_time(data)\n",
    "\n",
    "# 对天气进行编码\n",
    "'''\n",
    "对于数据中的天气描述，我们没有办法进行直接的处理。因此这里需要进行编码处理。\n",
    "在这次的处理中，我们按照天气的恶劣程度进行认为的编码，编码的程度从0到3 。分别\n",
    "对应由好到坏的天气状况。\n",
    "'''\n",
    "def weather_code(weather):\n",
    "    w_dict={}\n",
    "    w_dict['Clear']=0\n",
    "    w_dict['Clouds']=1\n",
    "    w_dict['Drizzle']=1\n",
    "    w_dict['Fog']=1\n",
    "    w_dict['Haze']=1\n",
    "    w_dict['Mist']=1\n",
    "    w_dict['Rain']=2\n",
    "    w_dict['Smoke']=2\n",
    "    w_dict['Snow']=3\n",
    "    w_dict['Squall']=3\n",
    "    w_dict['Thunderstorm']=3\n",
    "    return w_dict[weather]\n",
    "\n",
    "#对于天气描述进行编码\n",
    "'''\n",
    "这一部分我们将数据中所有的天气描述取出，通过由好到坏的程度进行编码，分别对应的是0-3 。 \n",
    "这里的编码是通过我们认为对比进行的。多次进行测试，得到一个比较优秀的编码顺序。这样的处理\n",
    "也是将天气描述部分进行数字化，可以方便我们模型的识别，同时增加我们的特征。在我们处理天气\n",
    "描述时，发现了数据集中没有的天气描述。对于这部分描述，我们统一返回为0.\n",
    "'''\n",
    "def wea_detail(weather):\n",
    "    wd_dict = {}\n",
    "    wd_dict['broken clouds']=0\n",
    "    wd_dict['drizzle']=1\n",
    "    wd_dict['few clouds']=0\n",
    "    wd_dict['fog']=1\n",
    "    wd_dict['freezing rain']=2\n",
    "    wd_dict['haze']=2\n",
    "    wd_dict['heavy intensity drizzle']=3\n",
    "    wd_dict['heavy intensity rain']=3\n",
    "    wd_dict['heavy snow']=3\n",
    "    wd_dict['light intensity drizzle']=1\n",
    "    wd_dict['light intensity shower rain']=1\n",
    "    wd_dict['light rain']=1\n",
    "    wd_dict['light rain and snow']=2\n",
    "    wd_dict['light snow']=1\n",
    "    wd_dict['mist']=1\n",
    "    wd_dict['moderate rain']=1\n",
    "    wd_dict['overcast clouds']=2\n",
    "    wd_dict['proximity shower rain']=2\n",
    "    wd_dict['proximity thunderstorm']=2\n",
    "    wd_dict['proximity thunderstorm with drizzle']=2\n",
    "    wd_dict['proximity thunderstorm with rain']=2\n",
    "    wd_dict['scattered clouds']=0\n",
    "    wd_dict['shower snow']=2\n",
    "    wd_dict['sky is clear']=0\n",
    "    wd_dict['Sky is Clear']=0\n",
    "    wd_dict['smoke']=2\n",
    "    wd_dict['snow']=2\n",
    "    wd_dict['SQUALLS']=2\n",
    "    wd_dict['thunderstorm']=3\n",
    "    wd_dict['thunderstorm with drizzle']=3\n",
    "    wd_dict['thunderstorm with heavy rain']=3\n",
    "    wd_dict['thunderstorm with light drizzle']=3\n",
    "    wd_dict['thunderstorm with light rain']=3\n",
    "    wd_dict['thunderstorm with rain']=3\n",
    "    wd_dict['very heavy rain']=3\n",
    "    if weather in wd_dict.keys():\n",
    "        return wd_dict[weather]\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# 在节假日字典里查找，当前日期是否属于节假日，如果属于返回1，否则返回0\n",
    "def holiday_lib(year,month,day,h_f_dict):\n",
    "    if [year,month,day] in h_f_dict:\n",
    "        return 1;\n",
    "    else:\n",
    "        return 0;\n",
    "\n",
    "# 获取权重值，在这个函数中，我们输入了时间和时间跨度。对应的权重为时间长度比上时间跨度\n",
    "#在函数最后我们得到一个权重列表的返回项\n",
    "def get_mean_w(data,nst,ned):\n",
    "    mean_w = []\n",
    "    for i in range(len(data)):\n",
    "        total = 0\n",
    "        for iloop in range(nst,ned):\n",
    "            total += data[i if i+iloop<0 or i+iloop >=len(data) else i+iloop]\n",
    "        mean_w.append(total/(ned-nst))\n",
    "    return mean_w\n",
    "\n",
    "# 对日期进行节假期和工作日的分类\n",
    "def get_vacand_week(data):\n",
    "    # 获取不同时间段\n",
    "    #获取工作日的前一天\n",
    "    data['weekday-1'] = [1 if datetime.datetime.strptime(str(i[0])+str(i[1])+str(i[2]),'%Y%m%d').weekday() in [0,1,2,3,4] else 0 for i in data.loc[:,['year','month','day']].values]\n",
    "    #获取工作日当天\n",
    "    data['weekday'] = [datetime.datetime.strptime(str(i[0])+str(i[1])+str(i[2]),'%Y%m%d').weekday()  for i in data.loc[:,['year','month','day']].values]\n",
    "    #获取假期时间\n",
    "    data['vac']    = [ holiday_lib(i[0],i[1],i[2],holiday_dict) for i in data.loc[:,['year','month','day']].values]\n",
    "    #获取节日时间\n",
    "    data['fes']    = [ holiday_lib(i[0],i[1],i[2],festival_dict) for i in data.loc[:,['year','month','day']].values]\n",
    "\n",
    "    # 对天气情况进行判断\n",
    "    '''\n",
    "    通过之前已经对天气的编码，这里可以直接对天气情况进行判断\n",
    "    由于之前已经对于两种天气描述进行编码，所以这里统一对于两种描述直接进行分类。\n",
    "    获取的值重新放到我们的列表中。\n",
    "    '''\n",
    "    data['wcode-too-bad']      = [ 1 if weather_code(i)==3 else 0 for i in data.loc[:,'weather_main'].values]\n",
    "    data['wcode-bad']      = [ 1 if weather_code(i)==2 else 0 for i in data.loc[:,'weather_main'].values]\n",
    "    data['wcode-mid']      = [ 1 if weather_code(i)==1 else 0 for i in data.loc[:,'weather_main'].values]\n",
    "    data['wcode-good']     = [ 1 if weather_code(i)==0 else 0 for i in data.loc[:,'weather_main'].values]\n",
    "    data['wdcode-too-bad']      = [ 1 if weather_code(i)==3 else 0 for i in data.loc[:,'weather_main'].values]\n",
    "    data['wdcode-bad']     = [ 1 if wea_detail(i)  ==2 else 0 for i in data.loc[:,'weather_description'].values]\n",
    "    data['wdcode-mid']     = [ 1 if wea_detail(i)  ==1 else 0 for i in data.loc[:,'weather_description'].values]\n",
    "    data['wdcode-good']    = [ 1 if wea_detail(i)  ==0 else 0 for i in data.loc[:,'weather_description'].values]\n",
    "    data['wdcode']    = [wea_detail(i)  for i in data.loc[:,'weather_description'].values]\n",
    "    data['wcode']     = [weather_code(i)  for i in data.loc[:,'weather_main'].values]\n",
    "\n",
    "    # 获取过往的天气情况\n",
    "    '''\n",
    "    因为数据具有一定的时效性，过去的天气状况很可能对现在或者将来对天气情况有影响。在这里\n",
    "    我们将过去五天内对天气情况都进行记录。即当前的天气情况和五天之内的天气情况\n",
    "    '''\n",
    "    data['wcode-1'] = [data['wcode'][i]   if i-1 <0 else data['wcode'][i-1] for i in range(len(data['wcode']))] \n",
    "    data['wcode-2'] = [data['wcode-1'][i] if i-2 <0 else data['wcode'][i-2] for i in range(len(data['wcode']))] \n",
    "    data['wcode-3'] = [data['wcode-2'][i] if i-3 <0 else data['wcode'][i-3] for i in range(len(data['wcode']))] \n",
    "    data['wcode-4'] = [data['wcode-3'][i] if i-4 <0 else data['wcode'][i-4] for i in range(len(data['wcode']))] \n",
    "    data['wcode-5'] = [data['wcode-4'][i] if i-5 <0 else data['wcode'][i-5] for i in range(len(data['wcode']))] \n",
    "\n",
    "    # 获取过去几个小时内的天气情况\n",
    "    #在这里我们获取的是不同时间长度的时间下，获得的权重\n",
    "    #过去2小时内的天气权重\n",
    "    data['wcode-mean']  = get_mean_w(data['wcode'].values,-1,2)\n",
    "    #过去12小时内的天气权重\n",
    "    data['wcode-mean0']  = get_mean_w(data['wcode'].values,-6,7)\n",
    "    #过去24小时内的天气权重\n",
    "    data['wcode-mean1'] = get_mean_w(data['wcode'].values,-12,13)\n",
    "    #过去48小时内的天气权重\n",
    "    data['wcode-mean2'] = get_mean_w(data['wcode'].values,-24,25)\n",
    "    #过去168小时内的天气权重\n",
    "    data['wcode-mean3'] = get_mean_w(data['wcode'].values,-84,85)\n",
    "\n",
    "    #过去6小时内的天气权重\n",
    "    data['wcode-mean-0']  = get_mean_w(data['wcode'].values,-6,1)\n",
    "    #过去12小时内的天气权重\n",
    "    data['wcode-mean-1'] = get_mean_w(data['wcode'].values,-12,1)\n",
    "    #过去24小时内的天气权重\n",
    "    data['wcode-mean-2'] = get_mean_w(data['wcode'].values,-24,1)\n",
    "    #过去84小时内的天气权重\n",
    "    data['wcode-mean-3'] = get_mean_w(data['wcode'].values,-84,1)\n",
    "\n",
    "    #未来6小时内的天气权重\n",
    "    data['wcode-mean+0']  = get_mean_w(data['wcode'].values,1,7)\n",
    "    #未来12小时内的天气权重\n",
    "    data['wcode-mean+1'] = get_mean_w(data['wcode'].values,1,13)\n",
    "    #未来24小时内的天气权重\n",
    "    data['wcode-mean+2'] = get_mean_w(data['wcode'].values,1,25)\n",
    "    #未来84小时内的天气权重\n",
    "    data['wcode-mean+3'] = get_mean_w(data['wcode'].values,1,85)\n",
    "    \n",
    "    # 将天气变化转化为变化梯度\n",
    "    data['temp_grade'] = [ (i[0]-240)//10 for i in data.loc[:,['temp']].values]\n",
    "    #将cloud的变化转化为变化梯度\n",
    "    data['cloud_grade'] = [ (i[0])//10 for i in data.loc[:,['clouds_all']].values]\n",
    "\n",
    "    # 单独获取节假日的权重变化\n",
    "    data['fes-mean']    = get_mean_w(data['fes'].values,-84,85)\n",
    "    return data\n",
    "\n",
    "data = get_vacand_week(data)\n",
    "\n",
    "'''\n",
    "在这里，我们按照不同的标准，分别聚合我们需要的数据。比如我们将每个月的流量的均值取出\n",
    "将每个月均值的平均权重取出。每个月流量的中位数，最大值和最小值取出。\n",
    "'''\n",
    "mean_per_month = data.groupby(['month','weekday','hour'])['traffic_volume'].mean()\n",
    "mean_per_month_w = data.groupby(['month','weekday-1','hour'])['traffic_volume'].mean()\n",
    "max_per_month = data.groupby(['month','weekday','hour'])['traffic_volume'].max()\n",
    "min_per_month = data.groupby(['month','weekday','hour'])['traffic_volume'].min()\n",
    "med_per_month = data.groupby(['month','weekday','hour'])['traffic_volume'].median()\n",
    "mean_per_weather = data.groupby(['wcode','weekday','hour'])['traffic_volume'].mean()\n",
    "\n",
    "# 获取历史数据\n",
    "def get_his(data):\n",
    "    #获取历史平均值\n",
    "    data['history_mean']  = [mean_per_month.loc[i[0],i[1],i[2]]   for i in data.loc[:,['month','weekday','hour']].values] \n",
    "    #获取历史平均权重\n",
    "    data['history_mean-w']  = [mean_per_month_w.loc[i[0],i[1],i[2]]   for i in data.loc[:,['month','weekday-1','hour']].values] \n",
    "    #获取历史最大值\n",
    "    data['history_max']   = [max_per_month.loc[i[0],i[1],i[2]]   for i in data.loc[:,['month','weekday','hour']].values] \n",
    "    #获取历史最小值\n",
    "    data['history_min']   = [min_per_month.loc[i[0],i[1],i[2]]   for i in data.loc[:,['month','weekday','hour']].values] \n",
    "    #获取历史中位数\n",
    "    data['history_med']   = [med_per_month.loc[i[0],i[1],i[2]]   for i in data.loc[:,['month','weekday','hour']].values] \n",
    "    #获取天气平均值\n",
    "    data['weather_mean'] = [mean_per_weather.loc[i[0],i[1],i[2]] for i in data.loc[:,['wcode','weekday','hour']].values] \n",
    "    #获取过去一小时的均值\n",
    "    data['history_mean_h']  = get_mean_w(data['history_mean'].values,-1,2)\n",
    "    #获取过去两小时的均值\n",
    "    data['history_mean_h1']  = get_mean_w(data['history_mean'].values,-2,3)\n",
    "    return data\n",
    "\n",
    "data = get_his(data)\n",
    "\n",
    "print (data.head())\n",
    "\n",
    "# 获取特征列表。在这里我们将我们所用的所有特征做成列表放在这里。\n",
    "features = ['month','day','temp','temp_grade','hour',\n",
    "            'history_mean','history_mean-w','history_mean_h','history_max','history_min',\n",
    "            'weekday','vac','fes','fes-mean','rain_1h','snow_1h','clouds_all',\n",
    "            'wcode','wdcode','weather_mean',\n",
    "            'wcode-mean','wcode-mean0','wcode-mean1','wcode-mean2','wcode-mean3',\n",
    "            'wcode-mean-0','wcode-mean-1','wcode-mean-2','wcode-mean-3',\n",
    "            'wcode-mean+0','wcode-mean+1','wcode-mean+2','wcode-mean+3'\n",
    "\n",
    "           ]\n",
    "\n",
    "len_f = len(features)\n",
    "array = data.loc[:,features+['traffic_volume']].values\n",
    "X = array[:,0:len_f]\n",
    "Y = array[:,len_f]\n",
    "n_splits = 5\n",
    "seed = 9\n",
    "\n",
    "#将数据集随机划分为训练集和验证集，比例为8:2\n",
    "X_train,X_valid,Y_train,Y_valid = train_test_split(X,Y,test_size = 0.2, random_state = seed)\n",
    "T_result = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import pickle\n",
    "from hyperopt import fmin, tpe, hp,space_eval,rand,Trials,partial,STATUS_OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立随机森林模型\n",
    "#本函数的输入为字典，字典中为各参数的索引\n",
    "#参数以修正异常值\n",
    "def svr_1(argsDict): \n",
    "    #子树的数量\n",
    "    n_estimators = argsDict['n_estimators'] + 3\n",
    "    #程序并行数量\n",
    "    n_jobs = argsDict['n_jobs'] + 7\n",
    "    #最大特征数\n",
    "    max_features = argsDict['max_features']\n",
    "    #最小样本数\n",
    "    min_samples_split = argsDict['min_samples_split'] + 10\n",
    "    #决策树最大深度\n",
    "    max_depth = argsDict['max_depth'] + 1\n",
    "    #叶子结点最小样本数\n",
    "    min_samples_leaf = argsDict['min_samples_leaf']\n",
    "    \n",
    "    #在这里我们初始化我们的随机森林模型\n",
    "    svr = SVR(n_estimators=n_estimators,n_jobs=n_jobs,max_features=max_features,min_samples_split=min_samples_split,max_depth=max_depth ,min_samples_leaf=min_samples_leaf)\n",
    "    #返回值为我们的交叉熵\n",
    "    return cross_val_score(svr, X_train, Y_train).mean()\n",
    "\n",
    "#创建我们的函数模型，用来检验当前参数是否为自由参数\n",
    "def f(params):\n",
    "    global best\n",
    "    acc = hyperopt_train_test(params)\n",
    "    if acc > best:\n",
    "        best = acc\n",
    "    print ('new best:', best, params)\n",
    "    return {'loss': -acc, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:09<00:00,  4.70s/trial, best loss: -0.00016186152796304754]\n",
      "{'max_depth': 35, 'max_features': 0.9348309058326543, 'min_samples_leaf': 0.4082075219106091, 'min_samples_split': 23, 'n_estimators': 187, 'n_jobs': 9}\n"
     ]
    }
   ],
   "source": [
    "# 创建搜索空间，在搜索空间中，我们定义参数和他的可行域\n",
    "space = {\"n_estimators\":hp.randint(\"n_estimators\", 200),\n",
    "         \"n_jobs\":hp.randint(\"n_jobs\",10),  \n",
    "         \"max_features\":hp.uniform(\"max_features\",0,1),  \n",
    "         \"min_samples_split\":hp.randint(\"min_samples_split\",40),\n",
    "         \"max_depth\":hp.randint(\"max_depth\",50),\n",
    "         \"min_samples_leaf\":hp.uniform(\"min_samples_leaf\",0,0.5)\n",
    "        }\n",
    "\n",
    "# 寻找最优参数\n",
    "algo = partial(tpe.suggest)\n",
    "best = fmin(svr_1,space,algo=algo,max_evals=2)\n",
    "print(best)\n",
    "#此时返回的best为一个拥有最优参数的字典，字典的索引就是我们搜索空间的索引\n",
    "model_svr = SVR(n_estimators=best['n_estimators'] + 3,n_jobs=int(best['n_jobs']) + 7,max_features=best['max_features'],\n",
    "                min_samples_split=best['min_samples_split'] + 10,max_depth=best['max_depth'] + 1 ,min_samples_leaf=best['min_samples_leaf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0010131003688543                                  \n",
      "-1.9396019153411834                                                            \n",
      "100%|██████████| 2/2 [00:00<00:00,  2.47trial/s, best loss: 1.0010131003688543]\n"
     ]
    }
   ],
   "source": [
    "# 创建XGBoost模型\n",
    "def GBM(argsDict):\n",
    "    max_depth = argsDict[\"max_depth\"] \n",
    "    n_estimators = argsDict['n_estimators'] \n",
    "    learning_rate = argsDict[\"learning_rate\"] \n",
    "    subsample = argsDict[\"subsample\"] \n",
    "    min_child_weight = argsDict[\"min_child_weight\"]\n",
    "\n",
    "    gbm = xgb.XGBRegressor(nthread = 20, #线程数\n",
    "                            max_depth=max_depth,  #最大深度\n",
    "                            n_estimators=n_estimators,   #树的数量\n",
    "                            learning_rate=learning_rate, #学习速率\n",
    "                            subsample=subsample,      #对样本采样数\n",
    "                            min_child_weight=min_child_weight,   #子叶权重\n",
    "                            max_delta_step = 50,  #最大迭代数\n",
    "                        )\n",
    "\n",
    "    #获取交叉熵\n",
    "    metric = cross_val_score(gbm,X_train,Y_train,cv=5).mean()\n",
    "    print(metric)\n",
    "    return -metric\n",
    "\n",
    "#定义搜索空间\n",
    "space = {\"max_depth\":hp.randint(\"max_depth\",15),\n",
    "         \"n_estimators\":hp.randint(\"n_estimators\",10),  \n",
    "         \"learning_rate\":hp.randint(\"learning_rate\",6),  \n",
    "         \"subsample\":hp.uniform(\"subsample\",0,1),\n",
    "         \"min_child_weight\":hp.randint(\"min_child_weight\",5), \n",
    "        }\n",
    "\n",
    "#定义搜索函数\n",
    "algo = partial(tpe.suggest,n_startup_jobs=10)\n",
    "\n",
    "best = fmin(GBM,space,algo=algo,max_evals=2)\n",
    "#使用最优化的参数重新定义模型\n",
    "model_xgboost = xgb.XGBRegressor(max_depth = best['max_depth'], n_estimators = best['n_estimators'], learning_rate = best['learning_rate'],\n",
    "                                subsample = best['subsample'], min_child_weight = best['min_child_weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 81.22trial/s, best loss: 327759908.95048726]\n"
     ]
    }
   ],
   "source": [
    "model_xgboost.fit(X_train, Y_train)\n",
    "X_text_1 = model_xgboost.predict(X_valid)\n",
    "model_svr.fit(X_train, Y_train)\n",
    "X_text_2 = model_svr.predict(X_valid)\n",
    "\n",
    "#对三个模型进行线性组合，通过验证集寻找最优参数\n",
    "def cross_k(argsDict):\n",
    "    cross = 0\n",
    "    for i in range(len(Y_valid)):\n",
    "        a = float(Y_valid[i]) - float(X_text_1[i])*argsDict['b'] - float(X_text_2[i])*argsDict['c'] \n",
    "        cross = cross + a ** 2\n",
    "    return(cross/len(Y_valid))\n",
    "\n",
    "#定义搜索空间，在数学上，空间满足叠加关系\n",
    "space = {\"b\":hp.uniform(\"b\",-0.3,1),\n",
    "         \"c\":hp.uniform(\"c\",-0.3,1)\n",
    "        }\n",
    "\n",
    "#定义搜索函数，寻找最优参数\n",
    "algo = partial(tpe.suggest,n_startup_jobs=10)  \n",
    "best = fmin(cross_k,space,algo=algo,max_evals=50)\n",
    "b_xgb = best['b']\n",
    "c_svr = best['c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n在我们的这个模型中，一共运用了三个模型。可以这么使用的原因是因为三个模型的本质都是树模型。\\n这个关系可以理解为随机森林和决策树的关系。但是随机森林的结果是明显好于决策树的。所以我们的\\n模型也可以理解为集成模型的再集成。在最后三个模型的拟合部分选择的是线性拟合，这样选择的原因是\\n时间模型按时序拟合效果通过线性拟合可以体现出来。时序（泊松分布）的拟合在回归模型中不能够很好的\\n体现数据集的连续性，只能作为离散点来处理，这样会降低我们的准确性。而我们最后的线性拟合中，在数学上\\n所有参数的和应该为1，但在我们模型中，可能不为1 。 这样的原因是在我们的实际处理中，所有特征不是绝对\\n相关的，有的特征关联性大，有的关联性小，还存在互斥特征，所以参数和不一定能为1.\\n'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "在我们的这个模型中，一共运用了三个模型。可以这么使用的原因是因为三个模型的本质都是树模型。\n",
    "这个关系可以理解为随机森林和决策树的关系。但是随机森林的结果是明显好于决策树的。所以我们的\n",
    "模型也可以理解为集成模型的再集成。在最后三个模型的拟合部分选择的是线性拟合，这样选择的原因是\n",
    "时间模型按时序拟合效果通过线性拟合可以体现出来。时序（泊松分布）的拟合在回归模型中不能够很好的\n",
    "体现数据集的连续性，只能作为离散点来处理，这样会降低我们的准确性。而我们最后的线性拟合中，在数学上\n",
    "所有参数的和应该为1，但在我们模型中，可能不为1 。 这样的原因是在我们的实际处理中，所有特征不是绝对\n",
    "相关的，有的特征关联性大，有的关联性小，还存在互斥特征，所以参数和不一定能为1.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
